{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocab Filter Test\n",
    "partiall\n",
    "full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17-object, Massive Memory is the correspodning eval dataset in science paper.\n",
    "64 classes after baby filter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from utils import load_baby_vocab, vocab_class_filter \n",
    "\n",
    "df = pd.read_excel('/home/Dataset/xueyi/KonkLab/17-objects/MM2-Ranks.xls')\n",
    "class_names = df['Category'].tolist()\n",
    "\n",
    "vocab_set = set(load_baby_vocab())\n",
    "\n",
    "filtered_classes = vocab_class_filter(class_names, vocab_set, match_type='full')\n",
    "print(len(filtered_classes))\n",
    "print(filtered_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from utils import load_baby_vocab, vocab_class_filter \n",
    "\n",
    "class_names = [\"siamese cat\", \"blue+whale\", \"toy_poodle\"]\n",
    "vocab_set = set(load_baby_vocab())\n",
    "filtered_classes = vocab_class_filter(class_names, vocab_set, match_type='full')\n",
    "print(filtered_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Trial Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "from datasets.object_dataset import KonkTrialDataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "dataset = KonkTrialDataset(trials_file='datasets/trials/object_trials_42.json', transform=transform)\n",
    "\n",
    "print(f'Dataset length: {len(dataset)}')\n",
    "\n",
    "sample_idx = 0\n",
    "imgs, label = dataset[sample_idx]\n",
    "\n",
    "print(f'Sample {sample_idx} - Label: {label}')\n",
    "print(f'Sample {sample_idx} - Images shape: {imgs.shape}')\n",
    "\n",
    "assert isinstance(imgs, torch.Tensor), \"Images should be a tensor\"\n",
    "assert imgs.shape == (4, 3, 224, 224), f\"Expected image shape (4, 3, 224, 224), but got {imgs.shape}\"\n",
    "\n",
    "combined_imgs = torch.cat([imgs[i] for i in range(imgs.shape[0])], dim=2)\n",
    "\n",
    "# 因为matplotlib预期图像的格式是 (高度, 宽度, 通道数)，我们需要调整张量的顺序\n",
    "combined_imgs = combined_imgs.permute(1, 2, 0)  # 重新排列维度为 (224, 896, 3)\n",
    "\n",
    "# 将图像数据的值归一化到 [0, 1]，因为它们可能已经被标准化处理过\n",
    "combined_imgs = combined_imgs.numpy()  # 转换为numpy数组\n",
    "combined_imgs = (combined_imgs - combined_imgs.min()) / (combined_imgs.max() - combined_imgs.min())\n",
    "\n",
    "# 使用matplotlib显示图像\n",
    "plt.figure(figsize=(10, 2.5))  # 设置图像显示大小\n",
    "plt.imshow(combined_imgs)\n",
    "plt.axis('off')  # 关闭坐标轴\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trial Predictor Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP...\n",
      "Successfully loaded CLIP-ViT-L/14\n",
      "Loading dataset: object-trial\n",
      "Sample 0 - Label: microwave\n"
     ]
    }
   ],
   "source": [
    "from data_utils import get_model, get_dataset\n",
    "\n",
    "device = 'cuda'\n",
    "model_name = 'clip'\n",
    "dataset = 'object-trial'\n",
    "trials_file = 'datasets/trials/object_img_5_3_42.json'\n",
    "model, transform = get_model(model_name, device)\n",
    "\n",
    "data = get_dataset(dataset_name=dataset, transform=transform, trials_file=trials_file,)\n",
    "\n",
    "sample_idx = 0\n",
    "imgs, label = data[sample_idx]\n",
    "print(f'Sample {sample_idx} - Label: {label}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "microwave\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarity: tensor([[[9.9902e-01],\n",
      "         [6.7186e-04],\n",
      "         [9.0003e-06],\n",
      "         [1.5104e-04]]], device='cuda:0', dtype=torch.float16)\n",
      "tensor([9.9902e-01, 6.7186e-04, 9.0003e-06, 1.5104e-04], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from models.zs_trial_predic import ZSTrialClassifier\n",
    " \n",
    "correct_pred = 0\n",
    "total_pred = 0\n",
    "correct_cls_pred = defaultdict(int)\n",
    "total_cls_predic = defaultdict(int)\n",
    "\n",
    "classifier = ZSTrialClassifier(model_name, model, device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    imgs = imgs.unsqueeze(0).to(device)\n",
    "    print(label)\n",
    "    batch_size, per_trial_img_num, channels, height, width = imgs.size()\n",
    "    imgs = imgs.view(-1, channels, height, width)\n",
    "    img_features = classifier.get_img_feature(imgs)  # [batch*4, 512]\n",
    "    img_features = img_features.view(batch_size, per_trial_img_num, -1)  \n",
    "    img_features = classifier.norm_features(img_features) # [batch, 4, 512]\n",
    "    txt_features = classifier.get_txt_feature(label)  # [batch, 512]\n",
    "    txt_features = classifier.norm_features(txt_features) \n",
    "    txt_features = txt_features.unsqueeze(1) # [batch, 1, 512]\n",
    "    \n",
    "    # Calculate the cosine similarity\n",
    "    similarity = (100.0 * img_features @ txt_features.transpose(-2, -1)).softmax(dim=1)  # [batch, 4, 1]\n",
    "    print(f\"similarity: {similarity}\")\n",
    "    similarity = similarity.squeeze(-1) # Remove the last dimension\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        simil = similarity[i]  # Get the similarity scores for the i-th item in the batch\n",
    "        print(simil)\n",
    "        predic_idx = simil.argmax().item()  # Find the index of the max similarity score for each trial\n",
    "        print(predic_idx)\n",
    "        if predic_idx == 0:  # gt is the first image\n",
    "            correct_pred += 1\n",
    "            correct_cls_pred[label[i]] += 1\n",
    "        total_pred += 1\n",
    "        total_cls_predic[label[i]] += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvcl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
