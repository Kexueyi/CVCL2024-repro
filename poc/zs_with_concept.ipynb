{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhanced Zero-shot Learning with Neuron-concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from data_utils import get_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_name = \"resnet50\"\n",
    "model, preprocess = get_model(model_name, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Register Hooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# activation with pooling mode\n",
    "def get_activation(outputs, mode):\n",
    "    '''\n",
    "    mode: how to pool activations: one of avg, max\n",
    "    for fc or ViT neurons does no pooling\n",
    "    '''\n",
    "    if mode=='avg':\n",
    "        def hook(model, input, output):\n",
    "            if len(output.shape)==4: #CNN layers\n",
    "                outputs.append(output.mean(dim=[2,3]).detach())\n",
    "            elif len(output.shape)==3: #ViT\n",
    "                outputs.append(output[:, 0].clone())\n",
    "            elif len(output.shape)==2: #FC layers\n",
    "                outputs.append(output.detach())\n",
    "    elif mode=='max':\n",
    "        def hook(model, input, output):\n",
    "            if len(output.shape)==4: #CNN layers\n",
    "                outputs.append(output.amax(dim=[2,3]).detach())\n",
    "            elif len(output.shape)==3: #ViT\n",
    "                outputs.append(output[:, 0].clone())\n",
    "            elif len(output.shape)==2: #FC layers\n",
    "                outputs.append(output.detach())\n",
    "    return hook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def per_img_activations(target_model, target_name, target_layers, dataset, batch_size, device='cuda', pool_mode='avg'):\n",
    "\n",
    "    all_features = {layer: [] for layer in target_layers}\n",
    "    hooks = {}\n",
    "\n",
    "    # register forward hook\n",
    "    for layer in target_layers:\n",
    "        module = dict(target_model.named_modules()).get(layer)\n",
    "        if module:\n",
    "            hooks[layer] = module.register_forward_hook(get_activation(all_features[layer], pool_mode))\n",
    "            print(f\"Hook registered for layer: {layer}\")\n",
    "        else:\n",
    "            print(f\"Warning: Layer '{layer}' does not exist in the model.\")\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(DataLoader(dataset, batch_size, num_workers=8, pin_memory=True)):\n",
    "            if \"cvcl\" in target_name:\n",
    "                _ = target_model.encode_image(images.to(device))\n",
    "            else:\n",
    "                _ = target_model(images.to(device))\n",
    "\n",
    "    # Remove \n",
    "    for layer in target_layers:\n",
    "        hooks[layer].remove()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"Activations saved and memory cleaned up.\")\n",
    "\n",
    "    # dict: {layername: activation tensor[batch_size, n_neurons]}\n",
    "    activations = {layer: torch.stack(all_features[layer]) for layer in target_layers}\n",
    "\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = preprocess(Image.open('data/toy_example_dataset_konka/muffin/img_9246-muffin.jpg'))\n",
    "data = torch.unsqueeze(img, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get activation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hook registered for layer: layer4\n",
      "Hook registered for layer: fc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:13<00:00, 13.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations saved and memory cleaned up.\n",
      "torch.Size([1, 1, 2048]) torch.Size([1, 1, 1000])\n"
     ]
    }
   ],
   "source": [
    "activations = per_img_activations(target_model=model, target_name=model_name, target_layers=target_layers, dataset=data, batch_size=200, device=device)\n",
    "\n",
    "print(activations['layer4'].shape,\n",
    "      activations['fc'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Top-k activationa and index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_values, top_k_indices = torch.topk(activations['fc'], k=10, dim=2) \n",
    "\n",
    "# flat tensor into list\n",
    "top_k_indices = top_k_indices[0].tolist()[0]\n",
    "top_k_values = top_k_values[0].tolist()[0]\n",
    "\n",
    "top_k_df = pd.DataFrame({\n",
    "    'unit': top_k_indices,\n",
    "    'value': top_k_values\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Zero-shot Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    layer  unit description  similarity      value\n",
      "6  layer4   930         pen    1.301636  10.217084\n",
      "8  layer4   962       phone    1.341309   9.900900\n",
      "5  layer4   911         car    0.707062   9.468554\n",
      "7  layer4   960    stroller    0.829865   9.288763\n",
      "0  layer4   415   microwave    0.744781   9.257796\n",
      "1  layer4   551    computer    0.882507   8.907595\n",
      "9  layer4   969        desk    3.426575   8.473521\n",
      "2  layer4   588       swing    2.122437   8.260846\n",
      "4  layer4   824    backpack    0.912323   8.117612\n",
      "3  layer4   738    backpack    0.702332   7.558163\n"
     ]
    }
   ],
   "source": [
    "# Load descriptions generated from CLIP-dissect\n",
    "descriptions = pd.read_csv('descriptions/res_konk_baby.csv')\n",
    "\n",
    "# Filter the descriptions to match the top_k indices with the descriptions\n",
    "filtered_descriptions = descriptions[descriptions.apply(lambda row: row['unit'] in top_k_indices and row['layer'] == 'layer4', axis=1)]\n",
    "\n",
    "# Merge the filtered descriptions with the top_k DataFrame\n",
    "matched_descriptions = pd.merge(filtered_descriptions, top_k_df, on='unit')\n",
    "\n",
    "# Sort the matched descriptions by 'value' in descending order\n",
    "matched_descriptions = matched_descriptions.sort_values(by='value', ascending=False)\n",
    "\n",
    "# Print the final DataFrame\n",
    "print(matched_descriptions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvcl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
