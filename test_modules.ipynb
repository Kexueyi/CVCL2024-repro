{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import set_seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from glob import glob\n",
    "import json\n",
    "\n",
    "class AnimalDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, class_file=None, baby_vocab=False, use_attr=False, top_n= None, continuous=True):\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.transform = transform\n",
    "        self.class_file = class_file\n",
    "        self.use_attr = use_attr\n",
    "        self.continuous = continuous\n",
    "        self.top_n = top_n  \n",
    "        # load all classes\n",
    "        self.full_classes = self.load_full_class_info()\n",
    "\n",
    "        self.attribute_file = self.load_attribute_file()\n",
    "        self.attribute_matrix = self.load_cls_attr_matrix()\n",
    "        self.vocab = self.load_vocab() if baby_vocab else {}\n",
    "        if baby_vocab:\n",
    "            self.filter_vocab() # updae full_classes, attribute_file, attribute_matrix\n",
    "\n",
    "        # Filter classes and prepare internal mappings\n",
    "        self.classes, self.index_map = self.filter_classes_and_attributes()\n",
    "        self.clean_cls_names = self.clean_class_names()\n",
    "        self.class_descriptions = self.generate_class_descriptions()\n",
    "\n",
    "        # Load images\n",
    "        self.img_paths, self.img_labels = self.load_images()\n",
    "\n",
    "    def load_vocab(self):\n",
    "        with open(\"multimodal/vocab.json\", 'r') as f:\n",
    "            return json.load(f)\n",
    "    \n",
    "    def filter_vocab(self):\n",
    "        self.attribute_file = self.attribute_file[self.attribute_file['attribute_name'].isin(self.vocab)]\n",
    "        valid_attribute_indices = self.attribute_file.index.tolist()\n",
    "\n",
    "        self.attribute_matrix = self.attribute_matrix[:, valid_attribute_indices]\n",
    "        # Filter classes based on the vocab\n",
    "        valid_class_names = set(self.full_classes['class_name']) & set(self.vocab)  # Ensure only classes in vocab are kept\n",
    "        self.full_classes = self.full_classes[self.full_classes['class_name'].isin(valid_class_names)]\n",
    "\n",
    "    def load_full_class_info(self):\n",
    "        class_path = self.root_dir / 'classes.txt'\n",
    "        return pd.read_csv(class_path, sep='\\t', header=None, names=['class_index', 'class_name'])\n",
    "\n",
    "    def load_attribute_file(self):\n",
    "        attr_path = self.root_dir / 'predicates.txt'\n",
    "        attr_data = pd.read_csv(attr_path, sep='\\t', header=None, names=['attribute_index', 'attribute_name'])\n",
    "        return attr_data.reset_index(drop=True)\n",
    "\n",
    "    def load_cls_attr_matrix(self):\n",
    "        matrix_file = 'predicate-matrix-continuous.txt' if self.continuous else 'predicate-matrix-binary.txt'\n",
    "        return np.genfromtxt(self.root_dir / matrix_file, dtype='float' if self.continuous else 'int')\n",
    "\n",
    "    def clean_class_names(self):\n",
    "        return [name.replace('+', ' ') for name in self.classes['class_name']]\n",
    "    \n",
    "    def filter_classes_and_attributes(self):\n",
    "        subset_path = self.root_dir / self.class_file\n",
    "        subset_classes = pd.read_csv(subset_path, sep='\\t', header=None, names=['class_name'])\n",
    "        filtered_classes = self.full_classes[self.full_classes['class_name'].isin(subset_classes['class_name'])]\n",
    "        return filtered_classes.reset_index(drop=True), {i: idx for i, idx in enumerate(filtered_classes['class_index'])}\n",
    "    \n",
    "    def generate_class_descriptions(self):\n",
    "        descriptions = {}\n",
    "        for idx, row in self.classes.iterrows():\n",
    "            full_index = row['class_index']\n",
    "            attr_vector = self.attribute_matrix[full_index - 1]\n",
    "            descriptions[full_index] = ', '.join(self.attributes_to_text(attr_vector, self.top_n))\n",
    "        return descriptions\n",
    "\n",
    "    def attributes_to_text(self, attributes_vector, top_n):\n",
    "        valid_indices = [i for i, name in enumerate(self.attribute_file['attribute_name']) if not self.vocab or name in self.vocab]\n",
    "        filtered_attributes = attributes_vector[valid_indices]\n",
    "        filtered_names = [self.attribute_file['attribute_name'].iloc[i] for i in valid_indices]\n",
    "\n",
    "        if self.continuous:\n",
    "            top_indices = np.argsort(filtered_attributes)[-top_n:]\n",
    "            return [filtered_names[i] for i in reversed(top_indices)]\n",
    "        else:\n",
    "            return [name for attr, name in zip(filtered_attributes, filtered_names) if attr == 1]\n",
    "\n",
    "    def load_images(self):\n",
    "        img_paths = []\n",
    "        img_labels = []\n",
    "        for idx, row in self.classes.iterrows():\n",
    "            class_folder = self.root_dir / 'JPEGImages' / row['class_name'].replace('+', ' ')\n",
    "            class_images = glob(str(class_folder / '*.jpg'))\n",
    "            img_paths.extend(class_images)\n",
    "            img_labels.extend([row['class_index']] * len(class_images))\n",
    "        return img_paths, img_labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        full_class_index = self.img_labels[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        if self.use_attr:\n",
    "            reverse_index_map = {v: k for k, v in self.index_map.items()}\n",
    "            continuous_index = reverse_index_map.get(full_class_index, -1)\n",
    "            attributes = self.attribute_matrix[continuous_index] if continuous_index != -1 else None\n",
    "            description = self.class_descriptions.get(full_class_index, \"\")\n",
    "            return image, full_class_index, description, attributes\n",
    "\n",
    "        return image, full_class_index\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP...\n",
      "Successfully loaded CLIP-ViT-L/14\n"
     ]
    }
   ],
   "source": [
    "from data_utils import get_model\n",
    "\n",
    "device = 'cuda'\n",
    "model_name = 'clip'\n",
    "model, preprocess = get_model(model_name, device)\n",
    "data = AnimalDataset(\"/home/Dataset/xueyi/Animals_with_Attributes2\", preprocess, 'classes.txt',True, True, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{7: 'fast, big, tail, brown, strong, ground, lean, black, white, furry, smart', 12: 'small, brown, ground, furry, black, gray, slow, lean, fast, tail, smart', 13: 'stripes, strong, big, ground, fast, orange, tail, furry, lean, bush, black', 16: 'brown, big, strong, ground, slow, furry, tail, smelly, fast, smart, gray', 19: 'big, gray, strong, ground, slow, tail, smelly, bush, smart, brown, white', 22: 'smart, fast, tail, furry, ground, small, brown, lean, orange, red, fish', 23: 'white, ground, furry, slow, black, smelly, gray, small, bush, big, tail', 27: 'tail, furry, small, gray, tree, brown, fast, ground, lean, smart, hands', 29: 'furry, ground, white, small, fast, tail, brown, gray, black, bush, smelly', 31: 'big, lean, ground, yellow, tail, bush, brown, fast, strong, slow, orange', 38: 'stripes, black, white, ground, fast, tail, big, bush, lean, furry, strong', 42: 'ground, smelly, big, tail, slow, brown, strong, gray, white, smart, black', 43: 'strong, big, furry, fast, tail, ground, bush, brown, yellow, smelly, smart', 44: 'small, tail, ground, white, fast, furry, gray, brown, smelly, black, bush', 49: 'big, tail, brown, ground, white, black, strong, slow, smelly, furry, gray'}\n"
     ]
    }
   ],
   "source": [
    "print(data.class_descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import clip\n",
    "\n",
    "class ZeroShotClassifier:\n",
    "    def __init__(self, model_name, model, device):\n",
    "        self.model_name = model_name\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.model.eval()\n",
    "\n",
    "    def get_txt_feature(self, clean_cls_name, cls_desc=None, prefix=\"\", use_attr=False):\n",
    "        if use_attr:\n",
    "            if cls_desc is None:\n",
    "                raise ValueError(\"cls_desc must be provided when use_attr is True.\")            \n",
    "            combined_texts = [f\"{prefix}{name}, {desc}\" for name, desc in zip(clean_cls_name, cls_desc.values())]\n",
    "            if \"cvcl\" in self.model_name:\n",
    "                print(combined_texts)  # 打印以验证正确性\n",
    "                text_tokens = [self.model.tokenize(text) for text in combined_texts]\n",
    "            elif \"clip\" in self.model_name:\n",
    "                print(combined_texts)  # 打印以验证正确性\n",
    "                text_inputs = clip.tokenize(combined_texts).to(self.device)\n",
    "        else:\n",
    "            texts = [f\"{prefix}{c}\" for c in clean_cls_name]\n",
    "            if \"cvcl\" in self.model_name:\n",
    "                text_tokens = [self.model.tokenize(c) for c in texts]\n",
    "            elif \"clip\" in self.model_name:\n",
    "                text_inputs = clip.tokenize(texts).to(self.device)\n",
    "\n",
    "        if \"cvcl\" in self.model_name:\n",
    "            text_inputs = torch.cat([txt[0] for txt in text_tokens]).to(self.device)\n",
    "            text_lens = torch.cat([txt[1] for txt in text_tokens]).to(self.device)\n",
    "            text_features = self.model.encode_text(text_inputs, text_lens)\n",
    "        elif \"clip\" in self.model_name:\n",
    "            text_features = self.model.encode_text(text_inputs)\n",
    "\n",
    "        normalized_text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
    "        return normalized_text_features\n",
    "        \n",
    "    def get_img_feature(self, dataloader):\n",
    "        img_features_list = []\n",
    "        all_labels_list = []\n",
    "        for data in tqdm(dataloader, desc=\"Encoding Images\"):\n",
    "            img, label = data[:2] # can return more than 2 items\n",
    "            img = img.to(self.device)\n",
    "            img_features = self.model.encode_image(img)\n",
    "            img_features_list.append(img_features)\n",
    "            all_labels_list.append(label.to(self.device)) \n",
    "        img_features_tensor = torch.cat(img_features_list)\n",
    "        norm_img_features = img_features_tensor / img_features_tensor.norm(dim=-1, keepdim=True)\n",
    "        all_labels_tensor = torch.cat(all_labels_list)\n",
    "        return norm_img_features, all_labels_tensor\n",
    "    \n",
    "    def compute_similarity(self, img_features, text_features):\n",
    "        similarity = (100.0 * img_features @ text_features.T).softmax(dim=-1)\n",
    "        return similarity\n",
    "\n",
    "    def predict_labels(self, similarity, index_map):\n",
    "        preds = similarity.argmax(dim=-1)  # 这里获取的是相对于文本特征索引的最大值索引\n",
    "        # 使用从数据集中获取的映射来转换预测的索引\n",
    "        mapped_preds = [index_map[pred.item()] for pred in preds]\n",
    "        return torch.tensor(mapped_preds, device=preds.device)\n",
    "    \n",
    "    def predict(self, dataloader, prefix=None, use_attr=False):\n",
    "        with torch.no_grad():\n",
    "            text_features = self.get_txt_feature(dataloader.dataset.clean_cls_names, dataloader.dataset.class_descriptions, prefix, use_attr)\n",
    "            img_features, all_labels = self.get_img_feature(dataloader)\n",
    "            similarity = self.compute_similarity(img_features, text_features)\n",
    "            index_map = dataloader.dataset.index_map\n",
    "            all_preds = self.predict_labels(similarity, index_map)  # Pass the full_class_indices here\n",
    "            similarities = similarity.max(dim=1)[0].cpu().numpy()  # max value of each row\n",
    "        return similarities, all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    class_index class_name\n",
      "0             7      horse\n",
      "1            12       mole\n",
      "2            13      tiger\n",
      "3            16      moose\n",
      "4            19   elephant\n",
      "5            22        fox\n",
      "6            23      sheep\n",
      "7            27   squirrel\n",
      "8            29     rabbit\n",
      "9            31    giraffe\n",
      "10           38      zebra\n",
      "11           42        pig\n",
      "12           43       lion\n",
      "13           44      mouse\n",
      "14           49        cow\n",
      "{7: 'fast, big, tail, brown, strong, ground, lean, black, white, furry, smart', 12: 'small, brown, ground, furry, black, gray, slow, lean, fast, tail, smart', 13: 'stripes, strong, big, ground, fast, orange, tail, furry, lean, bush, black', 16: 'brown, big, strong, ground, slow, furry, tail, smelly, fast, smart, gray', 19: 'big, gray, strong, ground, slow, tail, smelly, bush, smart, brown, white', 22: 'smart, fast, tail, furry, ground, small, brown, lean, orange, red, fish', 23: 'white, ground, furry, slow, black, smelly, gray, small, bush, big, tail', 27: 'tail, furry, small, gray, tree, brown, fast, ground, lean, smart, hands', 29: 'furry, ground, white, small, fast, tail, brown, gray, black, bush, smelly', 31: 'big, lean, ground, yellow, tail, bush, brown, fast, strong, slow, orange', 38: 'stripes, black, white, ground, fast, tail, big, bush, lean, furry, strong', 42: 'ground, smelly, big, tail, slow, brown, strong, gray, white, smart, black', 43: 'strong, big, furry, fast, tail, ground, bush, brown, yellow, smelly, smart', 44: 'small, tail, ground, white, fast, furry, gray, brown, smelly, black, bush', 49: 'big, tail, brown, ground, white, black, strong, slow, smelly, furry, gray'}\n"
     ]
    }
   ],
   "source": [
    "classifier = ZeroShotClassifier(model_name, model, device)\n",
    "print(data.classes)\n",
    "cls_desc = data.class_descriptions\n",
    "print(cls_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['horse, fast, big, tail, brown, strong, ground, lean, black, white, furry, smart', 'mole, small, brown, ground, furry, black, gray, slow, lean, fast, tail, smart', 'tiger, stripes, strong, big, ground, fast, orange, tail, furry, lean, bush, black', 'moose, brown, big, strong, ground, slow, furry, tail, smelly, fast, smart, gray', 'elephant, big, gray, strong, ground, slow, tail, smelly, bush, smart, brown, white', 'fox, smart, fast, tail, furry, ground, small, brown, lean, orange, red, fish', 'sheep, white, ground, furry, slow, black, smelly, gray, small, bush, big, tail', 'squirrel, tail, furry, small, gray, tree, brown, fast, ground, lean, smart, hands', 'rabbit, furry, ground, white, small, fast, tail, brown, gray, black, bush, smelly', 'giraffe, big, lean, ground, yellow, tail, bush, brown, fast, strong, slow, orange', 'zebra, stripes, black, white, ground, fast, tail, big, bush, lean, furry, strong', 'pig, ground, smelly, big, tail, slow, brown, strong, gray, white, smart, black', 'lion, strong, big, furry, fast, tail, ground, bush, brown, yellow, smelly, smart', 'mouse, small, tail, ground, white, fast, furry, gray, brown, smelly, black, bush', 'cow, big, tail, brown, ground, white, black, strong, slow, smelly, furry, gray']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding Images: 100%|██████████| 29/29 [01:35<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([16,  7, 16,  ..., 49, 49, 49], device='cuda:0')\n",
      "tensor([ 7,  7,  7,  ..., 49, 49, 49], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "dataloader = torch.utils.data.DataLoader(data, batch_size=512, shuffle=False, num_workers=4)\n",
    "similarities, predictions, labels = classifier.predict(dataloader, prefix=\"\", use_attr=True)\n",
    "print(predictions)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 7, 1: 12, 2: 13, 3: 16, 4: 19, 5: 22, 6: 23, 7: 27, 8: 29, 9: 31, 10: 38, 11: 42, 12: 43, 13: 44, 14: 49}\n",
      "    class_index class_name\n",
      "0             7      horse\n",
      "1            12       mole\n",
      "2            13      tiger\n",
      "3            16      moose\n",
      "4            19   elephant\n",
      "5            22        fox\n",
      "6            23      sheep\n",
      "7            27   squirrel\n",
      "8            29     rabbit\n",
      "9            31    giraffe\n",
      "10           38      zebra\n",
      "11           42        pig\n",
      "12           43       lion\n",
      "13           44      mouse\n",
      "14           49        cow\n",
      "['horse', 'mole', 'tiger', 'moose', 'elephant', 'fox', 'sheep', 'squirrel', 'rabbit', 'giraffe', 'zebra', 'pig', 'lion', 'mouse', 'cow']\n"
     ]
    }
   ],
   "source": [
    "print(data.index_map)\n",
    "print(data.classes)\n",
    "print(data.clean_cls_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.04%\n"
     ]
    }
   ],
   "source": [
    "from utils import calculate_accuracy\n",
    "acc, _ = calculate_accuracy(predictions, labels)\n",
    "print(f\"Accuracy: {acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Top N?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CVCL...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.8 to v2.2.1. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../.cache/huggingface/hub/models--wkvong--cvcl_s_dino_resnext50_embedding/snapshots/fe96aa69683bad69e5dd5195fc874a3edb8cb691/cvcl_s_dino_resnext50_embedding.ckpt`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded CVCL-resnext50\n",
      "Tokens: tensor([[   2,  381,    5,  531,    5,  104,    5,  720,    5,  290,    5, 1552,\n",
      "            5,  795,    5, 1259,    5, 1915,    5,  998,    5,  416,    5,  450,\n",
      "            3]])\n",
      "Token lengths: tensor([25])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xke001/miniconda3/envs/cvcl/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:199: Attribute 'vision_encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['vision_encoder'])`.\n",
      "/home/xke001/miniconda3/envs/cvcl/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:199: Attribute 'text_encoder' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['text_encoder'])`.\n"
     ]
    }
   ],
   "source": [
    "model, _  = get_model('cvcl_res', device)\n",
    "description = \"horse,fast,big,tail,brown,strong,ground,lean,smart,furry,white,black\"\n",
    "tokens, token_lengths = model.tokenize(description)\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token lengths: {token_lengths}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    attribute_index attribute_name\n",
      "0                 1          black\n",
      "1                 2          white\n",
      "2                 3           blue\n",
      "3                 4          brown\n",
      "4                 5           gray\n",
      "5                 6         orange\n",
      "6                 7            red\n",
      "7                 8         yellow\n",
      "10               11        stripes\n",
      "11               12          furry\n",
      "14               15            big\n",
      "15               16          small\n",
      "17               18           lean\n",
      "19               20          hands\n",
      "25               26           tail\n",
      "33               34         smelly\n",
      "39               40           fast\n",
      "40               41           slow\n",
      "41               42         strong\n",
      "51               52           fish\n",
      "67               68           bush\n",
      "73               74          ocean\n",
      "74               75         ground\n",
      "75               76          water\n",
      "76               77           tree\n",
      "77               78           cave\n",
      "80               81          smart\n"
     ]
    }
   ],
   "source": [
    "print(data.attribute_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Attributes and their values:\n",
      "smart: 37.28\n",
      "furry: 40.58\n",
      "white: 42.91\n",
      "black: 44.9\n",
      "lean: 47.96\n",
      "ground: 56.52\n",
      "strong: 69.13\n",
      "brown: 69.41\n",
      "tail: 70.42\n",
      "big: 71.5\n",
      "fast: 81.68\n"
     ]
    }
   ],
   "source": [
    "class_index = 7  # 1-based index, 用于选取类似“horse”这样的类\n",
    "\n",
    "data = AnimalDataset(\"/home/Dataset/xueyi/Animals_with_Attributes2\", preprocess, 'classes.txt', True, True, True)\n",
    "attribute_vector = data.attribute_matrix[class_index - 1]\n",
    "\n",
    "# 获取属性值的索引，这次只取最高的8个或10个值\n",
    "top_indices = np.argsort(attribute_vector)[-11:]  # 可以修改这里的数值为-8或-10\n",
    "top_values = attribute_vector[top_indices]\n",
    "# 确保属性名称正确地对应于这些索引\n",
    "top_attribute_names = [data.attribute_file['attribute_name'].iloc[i] for i in top_indices]\n",
    "\n",
    "# 打印结果，确保以降序打印属性\n",
    "print(\"Top Attributes and their values:\")\n",
    "for name, value in zip(top_attribute_names, top_values):\n",
    "    print(f\"{name}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "more attri more noise? top11=>Accuracy: 6.84% top7=>8.64% \n",
    "no attri: 8+\n",
    "no baby: 4+\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cvcl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
